{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir save_model save_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import environment\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대로 실행할 에피소드 수를 설정합니다.\n",
    "EPISODES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카트폴 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        '''\n",
    "        구글 colab에서는 아래 render를 True로 만들면 실행이 안됩니다.\n",
    "        '''\n",
    "        self.render = True\n",
    "\n",
    "        '''\n",
    "        저장해 놓은 신경망 모델을 가져올 지 선택합니다. (mountaincar_trainded.h5)\n",
    "        훈련을 중간에 중단해 놓았다가 다시 시작하려면 아래를 True로 바꾸고 실행하시면 됩니다.\n",
    "        '''\n",
    "        self.load_model = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        '''\n",
    "        일단 None이라고 되어있는 부분 위주로 수정해주세요. (다른 것들 잘못 건드시면 안될수도 있음)\n",
    "        아래 8개 하이퍼파라미터(maxlen 포함)는 cartpole_dqn 예제 그대로 복사하셔도 되고, 좀 수정하셔도 됩니다.\n",
    "        '''\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.1\n",
    "        self.epsilon_min = 0.10\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 100\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 10000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.action_buffer = []\n",
    "        \n",
    "        # 모델과 타깃 모델 생성\n",
    "        '''\n",
    "        아마 그냥 실행하시면 오류가 날텐데\n",
    "        build_model을 완성하시면 오류가 사라집니다.\n",
    "        '''\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_state_dict(torch.load(\n",
    "                './save_model/tank_dqn_14.bin'))\n",
    "\n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.action_size),\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        # 버퍼에 남으면 버퍼 수행\n",
    "        if len(self.action_buffer) > 0:\n",
    "            return torch.LongTensor([[self.action_buffer.pop()]])\n",
    "\n",
    "        # 휴리스틱 가능하면 휴리스틱\n",
    "        buffer = env.try_to_kill()\n",
    "        if buffer:\n",
    "            buffer.reverse()\n",
    "            self.action_buffer = buffer\n",
    "            return torch.LongTensor([[self.action_buffer.pop()]])\n",
    "        \n",
    "        # 아니면 DQN\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            legal_actions = env.legal_actions()\n",
    "            actions = []\n",
    "            if 3 in legal_actions:\n",
    "                actions.append(3)\n",
    "            if 4 in legal_actions:\n",
    "                actions.append(4)\n",
    "            if 5 in legal_actions:\n",
    "                actions.append(5)\n",
    "            if 6 in legal_actions:\n",
    "                actions.append(6)\n",
    "            if 7 in legal_actions:\n",
    "                actions.append(7)\n",
    "            return torch.LongTensor([[random.choice(actions)]])\n",
    "        else:\n",
    "            # 모델로부터 행동 산출\n",
    "            action = self.model(state).data.max(1)[1] # 0~4\n",
    "            action = action + 3 # 3~7\n",
    "            return action.view(1, 1)\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        next_state = torch.FloatTensor([next_state])\n",
    "        done = torch.FloatTensor([done])\n",
    "\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "        dones = torch.cat(dones)\n",
    "\n",
    "        # 현재 상태에 대한 모델의 큐함수\n",
    "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "        current_q = self.model(states).gather(1, actions-3)\n",
    "        max_next_q = self.target_model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (self.discount_factor * max_next_q)\n",
    "\n",
    "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 5 is out of bounds for dimension 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\minwoo\\Desktop\\K-AI Challenge\\Final\\tank_dqn.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000015?line=8'>9</a>\u001b[0m dones\u001b[39m.\u001b[39mgather\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000015?line=9'>10</a>\u001b[0m \u001b[39m# 현재 상태에 대한 모델의 큐함수\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000015?line=10'>11</a>\u001b[0m \u001b[39m# 다음 상태에 대한 타깃 모델의 큐함수\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000015?line=11'>12</a>\u001b[0m current_q \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mmodel(states)\u001b[39m.\u001b[39;49mgather(\u001b[39m1\u001b[39;49m, actions)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index 5 is out of bounds for dimension 1 with size 5"
     ]
    }
   ],
   "source": [
    "# batch = random.sample(agent.memory, agent.batch_size)\n",
    "# states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "# states = torch.cat(states)\n",
    "# actions = torch.cat(actions)\n",
    "# rewards = torch.cat(rewards)\n",
    "# next_states = torch.cat(next_states)\n",
    "# dones = torch.cat(dones)\n",
    "\n",
    "# # 현재 상태에 대한 모델의 큐함수\n",
    "# # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "# current_q = agent.model(states).gather(1, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "except:\n",
    "    pass\n",
    "env = environment.Environment()\n",
    "\n",
    "state_size = len(env._get_state())\n",
    "action_size = 5 # 이동, 종료만 인정\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "scores, episodes, epsilons, max_poss = [], [], [], []\n",
    "\n",
    "ip = get('http://api.ipify.org').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndone = False\\nscore = 0\\nenv.start(ip=ip)\\nstate = env.reset()\\nagent.epsilon = 0.09\\n\\nwhile not done:\\n    # env.render()\\n    \\n    state = torch.FloatTensor([state])\\n    action = agent.get_action(state)\\n    \\n    next_state, reward, done, info = env.step(action)\\n    if info['action'] == 7:\\n        agent.action_buffer.clear()\\n    \\n    score += reward\\n    state = next_state\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 성공한 후 코드\n",
    "# 이 셀 실행하기 전 load.model = True로 해주고, 로드할 파일 명도 적절히 바꿔야 함\n",
    "'''\n",
    "done = False\n",
    "score = 0\n",
    "env.start(ip=ip)\n",
    "state = env.reset()\n",
    "agent.epsilon = 0.09\n",
    "\n",
    "while not done:\n",
    "    # env.render()\n",
    "    \n",
    "    state = torch.FloatTensor([state])\n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    if info['action'] == 7:\n",
    "        agent.action_buffer.clear()\n",
    "    \n",
    "    score += reward\n",
    "    state = next_state\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env._get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip: 39.120.243.239\n",
      "session create..done!\n",
      "join..done!\n",
      "game start..done!\n",
      "28571 -> move east\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28571 -> turn end\n",
      "\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28554 -> move south\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28554 -> turn end\n",
      "\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28537 -> move north\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28537 -> turn end\n",
      "\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28520 -> turn end\n",
      "\n",
      "wait enemy\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "our: 4, enemy:4\n",
      "turn 3 \n",
      "\n",
      "28571 -> move north\n",
      "assertion i, j 8 31\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28571 -> move south\n",
      "assertion i, j 9 31\n",
      "assertion i, j 10 31\n",
      "28571 -> move south\n",
      "assertion i, j 10 31\n",
      "28571 -> turn end\n",
      "\n",
      "assertion i, j 10 31\n",
      "28554 -> move east\n",
      "assertion i, j 10 31\n",
      "28554 -> move east\n",
      "assertion i, j 10 31\n",
      "28554 -> attack\n",
      "hit!\n",
      "assertion i, j 10 31\n",
      "illegal action\n",
      "28554 -> turn end\n",
      "\n",
      "assertion i, j 10 31\n",
      "28537 -> move south\n",
      "assertion i, j 10 31\n",
      "28537 -> move north\n",
      "assertion i, j 10 31\n",
      "28537 -> move east\n",
      "assertion i, j 10 31\n",
      "28537 -> move north\n",
      "assertion i, j 10 31\n",
      "28537 -> move south\n",
      "assertion i, j 10 31\n",
      "28537 -> turn end\n",
      "\n",
      "assertion i, j 10 31\n",
      "28520 -> move east\n",
      "assertion i, j 10 31\n",
      "28520 -> move south\n",
      "assertion i, j 10 31\n",
      "28520 -> turn end\n",
      "\n",
      "wait enemy\n",
      "assertion i, j 10 31\n",
      "our: 3, enemy:4\n",
      "turn 5 \n",
      "\n",
      "28571 -> move south\n",
      "assertion i, j 31 27\n",
      "28571 -> move west\n",
      "assertion i, j 31 26\n",
      "28571 -> move south\n",
      "assertion i, j 31 26\n",
      "28571 -> turn end\n",
      "\n",
      "28554 -> turn end\n",
      "\n",
      "assertion i, j 31 26\n",
      "28537 -> move west\n",
      "assertion i, j 31 26\n",
      "28537 -> attack\n",
      "hit!\n",
      "28537 -> attack\n",
      "hit!\n",
      "assertion i, j 31 26\n",
      "28537 -> turn end\n",
      "\n",
      "assertion i, j 31 26\n",
      "28520 -> move east\n",
      "28520 -> move east\n",
      "assertion i, j 31 26\n",
      "28520 -> move east\n",
      "assertion i, j 31 26\n",
      "28520 -> move north\n",
      "illegal action\n",
      "28520 -> turn end\n",
      "\n",
      "wait enemy\n",
      "assertion i, j 31 26\n",
      "our: 3, enemy:4\n",
      "turn 7 \n",
      "\n",
      "28571 -> move west\n",
      "28571 -> turn end\n",
      "\n",
      "28554 -> turn end\n",
      "\n",
      "28537 -> move north\n",
      "assertion i, j 31 25\n",
      "28537 -> move east\n",
      "assertion i, j 31 25\n",
      "28537 -> move west\n",
      "assertion i, j 31 25\n",
      "28537 -> move south\n",
      "28537 -> move east\n",
      "assertion i, j 31 25\n",
      "28537 -> turn end\n",
      "\n",
      "28520 -> turn end\n",
      "\n",
      "wait enemy\n",
      "episode: 0 | score: 50.00000 | memory length: 45 | epsilon: 1.00000\n",
      "{'action': tensor(7)}\n",
      "\n",
      " ****************************** \n",
      "\n",
      "ip: 39.120.243.239\n",
      "session create..done!\n",
      "join..done!\n",
      "game start..done!\n",
      "93054 -> move north\n",
      "93054 -> move south\n",
      "93054 -> turn end\n",
      "\n",
      "93037 -> move north\n",
      "93037 -> move east\n",
      "93037 -> move west\n",
      "93037 -> turn end\n",
      "\n",
      "93020 -> move south\n",
      "93020 -> move south\n",
      "93020 -> move south\n",
      "93020 -> move east\n",
      "93020 -> move west\n",
      "93020 -> turn end\n",
      "\n",
      "93003 -> move east\n",
      "93003 -> move west\n",
      "93003 -> move south\n",
      "93003 -> move east\n",
      "93003 -> move south\n",
      "illegal action\n",
      "93003 -> turn end\n",
      "\n",
      "wait enemy\n",
      "our: 4, enemy:4\n",
      "turn 3 \n",
      "\n",
      "93054 -> move west\n",
      "93054 -> move south\n",
      "93054 -> move east\n",
      "93054 -> move west\n",
      "93054 -> move south\n",
      "93054 -> turn end\n",
      "\n",
      "93037 -> turn end\n",
      "\n",
      "93020 -> move east\n",
      "93020 -> turn end\n",
      "\n",
      "93003 -> turn end\n",
      "\n",
      "wait enemy\n",
      "our: 4, enemy:4\n",
      "turn 5 \n",
      "\n",
      "93054 -> move east\n",
      "93054 -> move west\n",
      "93054 -> move north\n",
      "93054 -> move west\n",
      "93054 -> move north\n",
      "93054 -> turn end\n",
      "\n",
      "93037 -> move west\n",
      "93037 -> move south\n",
      "93037 -> turn end\n",
      "\n",
      "93020 -> move east\n",
      "93020 -> move west\n",
      "93020 -> move south\n",
      "93020 -> move east\n",
      "93020 -> move south\n",
      "93020 -> turn end\n",
      "\n",
      "93003 -> turn end\n",
      "\n",
      "wait enemy\n",
      "our: 4, enemy:4\n",
      "turn 7 \n",
      "\n",
      "93054 -> move south\n",
      "93054 -> move west\n",
      "93054 -> move north\n",
      "93054 -> move south\n",
      "93054 -> move north\n",
      "93054 -> turn end\n",
      "\n",
      "93037 -> move east\n",
      "93037 -> move east\n",
      "93037 -> move north\n",
      "93037 -> move east\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -3 is out of bounds for dimension 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\minwoo\\Desktop\\K-AI Challenge\\Final\\tank_dqn.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000006?line=17'>18</a>\u001b[0m     agent\u001b[39m.\u001b[39mappend_sample(state, action, reward, next_state, done)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000006?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtrain_start:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000006?line=20'>21</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000006?line=22'>23</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000006?line=23'>24</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "\u001b[1;32mc:\\Users\\minwoo\\Desktop\\K-AI Challenge\\Final\\tank_dqn.ipynb Cell 4'\u001b[0m in \u001b[0;36mDQNAgent.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=124'>125</a>\u001b[0m dones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(dones)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=126'>127</a>\u001b[0m \u001b[39m# 현재 상태에 대한 모델의 큐함수\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=127'>128</a>\u001b[0m \u001b[39m# 다음 상태에 대한 타깃 모델의 큐함수\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=128'>129</a>\u001b[0m current_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(states)\u001b[39m.\u001b[39;49mgather(\u001b[39m1\u001b[39;49m, actions\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=129'>130</a>\u001b[0m max_next_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model(next_states)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/minwoo/Desktop/K-AI%20Challenge/Final/tank_dqn.ipynb#ch0000003?line=130'>131</a>\u001b[0m expected_q \u001b[39m=\u001b[39m rewards \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount_factor \u001b[39m*\u001b[39m max_next_q)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index -3 is out of bounds for dimension 1 with size 5"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    env.start(ip=ip)\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        \n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if info['action'] == 7:\n",
    "            agent.action_buffer.clear()\n",
    "        \n",
    "        if action.item() in [3, 4, 5, 6, 7]:\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(agent.memory) >= agent.train_start:\n",
    "            agent.train_model()\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            pylab.savefig(\"./save_graph/tank_dqn.png\")\n",
    "            \n",
    "            print(f\"episode: {e} | score: {score:.5f} | memory length: {len(agent.memory)} | epsilon: {agent.epsilon:.5f}\")\n",
    "            \n",
    "            print(info)\n",
    "            print('\\n','*'*30,'\\n')\n",
    "            torch.save(agent.model.state_dict(),\n",
    "                        f\"./save_model/tank_dqn_{e}.bin\")\n",
    "    \n",
    "    if len(agent.memory) >= agent.train_start and agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon -= agent.epsilon_decay"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
