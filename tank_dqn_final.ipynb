{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir save_model save_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import environment\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대로 실행할 에피소드 수를 설정합니다.\n",
    "EPISODES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카트폴 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        '''\n",
    "        구글 colab에서는 아래 render를 True로 만들면 실행이 안됩니다.\n",
    "        '''\n",
    "        self.render = True\n",
    "\n",
    "        '''\n",
    "        저장해 놓은 신경망 모델을 가져올 지 선택합니다. (mountaincar_trainded.h5)\n",
    "        훈련을 중간에 중단해 놓았다가 다시 시작하려면 아래를 True로 바꾸고 실행하시면 됩니다.\n",
    "        '''\n",
    "        self.load_model = True\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        '''\n",
    "        일단 None이라고 되어있는 부분 위주로 수정해주세요. (다른 것들 잘못 건드시면 안될수도 있음)\n",
    "        아래 8개 하이퍼파라미터(maxlen 포함)는 cartpole_dqn 예제 그대로 복사하셔도 되고, 좀 수정하셔도 됩니다.\n",
    "        '''\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 0.0\n",
    "        # self.epsilon_decay = 0.1\n",
    "        # self.epsilon_min = 0.10\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 100\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 10000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.action_buffer = []\n",
    "        \n",
    "        # 모델과 타깃 모델 생성\n",
    "        '''\n",
    "        아마 그냥 실행하시면 오류가 날텐데\n",
    "        build_model을 완성하시면 오류가 사라집니다.\n",
    "        '''\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_state_dict(torch.load(\n",
    "                './save_model/tank_dqn_14.bin'))\n",
    "\n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.action_size),\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        # 버퍼에 남으면 버퍼 수행\n",
    "        if len(self.action_buffer) > 0:\n",
    "            return torch.LongTensor([[self.action_buffer.pop()]])\n",
    "\n",
    "        # 휴리스틱 가능하면 휴리스틱\n",
    "        buffer = env.try_to_kill()\n",
    "        if buffer:\n",
    "            buffer.reverse()\n",
    "            self.action_buffer = buffer\n",
    "            return torch.LongTensor([[self.action_buffer.pop()]])\n",
    "        \n",
    "        # 아니면 DQN\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            legal_actions = env.legal_actions()\n",
    "            actions = []\n",
    "            if 3 in legal_actions:\n",
    "                actions.append(3)\n",
    "            if 4 in legal_actions:\n",
    "                actions.append(4)\n",
    "            if 5 in legal_actions:\n",
    "                actions.append(5)\n",
    "            if 6 in legal_actions:\n",
    "                actions.append(6)\n",
    "            if 7 in legal_actions:\n",
    "                actions.append(7)\n",
    "            return torch.LongTensor([[random.choice(actions)]])\n",
    "        else:\n",
    "            # 모델로부터 행동 산출\n",
    "            action = self.model(state).data.max(1)[1] # 0~4\n",
    "            action = action + 3 # 3~7\n",
    "            return action.view(1, 1)\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        next_state = torch.FloatTensor([next_state])\n",
    "        done = torch.FloatTensor([done])\n",
    "\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "        dones = torch.cat(dones)\n",
    "\n",
    "        # 현재 상태에 대한 모델의 큐함수\n",
    "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "        current_q = self.model(states).gather(1, actions-3)\n",
    "        max_next_q = self.target_model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (self.discount_factor * max_next_q)\n",
    "\n",
    "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = random.sample(agent.memory, agent.batch_size)\n",
    "# states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "# states = torch.cat(states)\n",
    "# actions = torch.cat(actions)\n",
    "# rewards = torch.cat(rewards)\n",
    "# next_states = torch.cat(next_states)\n",
    "# dones = torch.cat(dones)\n",
    "\n",
    "# # 현재 상태에 대한 모델의 큐함수\n",
    "# # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "# current_q = agent.model(states).gather(1, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del env\n",
    "except:\n",
    "    pass\n",
    "env = environment.Environment()\n",
    "\n",
    "state_size = len(env._get_state())\n",
    "action_size = 5 # 이동, 종료만 인정\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "scores, episodes, epsilons, max_poss = [], [], [], []\n",
    "\n",
    "ip = get('http://api.ipify.org').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성공한 후 코드\n",
    "# 이 셀 실행하기 전 load.model = True로 해주고, 로드할 파일 명도 적절히 바꿔야 함\n",
    "'''\n",
    "done = False\n",
    "score = 0\n",
    "env.start(ip=ip)\n",
    "state = env.reset()\n",
    "agent.epsilon = 0.09\n",
    "\n",
    "while not done:\n",
    "    # env.render()\n",
    "    \n",
    "    state = torch.FloatTensor([state])\n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    if info['action'] == 7:\n",
    "        agent.action_buffer.clear()\n",
    "    \n",
    "    score += reward\n",
    "    state = next_state\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env._get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    env.start(ip=ip)\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        \n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if info['action'] == 7:\n",
    "            agent.action_buffer.clear()\n",
    "        \n",
    "        # if action.item() in [3, 4, 5, 6, 7]:\n",
    "            # agent.append_sample(state, action, reward, next_state, done)\n",
    "        \n",
    "        # if len(agent.memory) >= agent.train_start:\n",
    "            # agent.train_model()\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            # scores.append(score)\n",
    "            # episodes.append(e)\n",
    "            # pylab.plot(episodes, scores, 'b')\n",
    "            # pylab.savefig(\"./save_graph/tank_dqn.png\")\n",
    "            \n",
    "            # print(f\"episode: {e} | score: {score:.5f} | memory length: {len(agent.memory)} | epsilon: {agent.epsilon:.5f}\")\n",
    "            \n",
    "            print(info)\n",
    "            print('\\n','*'*30,'\\n')\n",
    "            # torch.save(agent.model.state_dict(),\n",
    "                        # f\"./save_model/tank_dqn_{e}.bin\")\n",
    "    \n",
    "    # if len(agent.memory) >= agent.train_start and agent.epsilon > agent.epsilon_min:\n",
    "        # agent.epsilon -= agent.epsilon_decay"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
